{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an extension of GridWorld_Env-3 \n",
    "# In this extension I have made following changes\n",
    "\n",
    "# 1. Made it general for X*Y grid example\n",
    "# 2. Give a heavy penalty for agent for trying to run out of the grid, i.e. hitting the boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld_Env:\n",
    "    def __init__(self, hor, ver):\n",
    "        self.actions = [\"left\", \"up\", \"right\", \"down\"] # 0=Left, 1=Up, 2=right, 3=Down \n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.MAX_HOR_VAL = hor-1\n",
    "        self.MAX_VER_VAL = ver-1\n",
    "        self.done = False\n",
    "        self.episode_length = 0\n",
    "        self.no_operation = False\n",
    "        self.state_observation = [self.x, self.y]\n",
    "        \n",
    "    def reset(self):\n",
    "        self.done = False\n",
    "        self.episode_length = 0\n",
    "        self.x, self.y = 0, 0\n",
    "        self.state_observation = [self.x, self.y]\n",
    "        return [self.x, self.y]\n",
    "    \n",
    "    def action_space(self):\n",
    "        return self.actions\n",
    "  \n",
    "    def step(self, action):\n",
    "        if self.state_observation == [self.MAX_HOR_VAL, self.MAX_VER_VAL]:\n",
    "            self.done = True\n",
    "            self.no_operation = False\n",
    "            return np.array(self.state_observation), self.reward, self.done, self.no_operation, self.episode_length\n",
    "        elif self.episode_length > 200:\n",
    "            self.done = True\n",
    "            self.no_operation = True\n",
    "            return np.array(self.state_observation), self.reward, self.done, self.no_operation, self.episode_length\n",
    "        \n",
    "        self.action = action\n",
    "        self.reward = self.get_reward()\n",
    "        self.state_observation = self.take_action()\n",
    "        self.episode_length += 1\n",
    "        self.no_operation = False\n",
    "        \n",
    "        if(self.episode_length >= 200):\n",
    "            self.done = True\n",
    "        \n",
    "        return np.array(self.state_observation), self.reward, self.done, self.no_operation, self.episode_length\n",
    "    \n",
    "    def get_reward(self):\n",
    "        '''\n",
    "        Return value : rewards\n",
    "        Input argument. \n",
    "        '''\n",
    "        if (self.x == 0 and self.action == \"left\") or (self.x == self.MAX_HOR_VAL and self.action == \"right\"):\n",
    "            return -2\n",
    "        elif (self.y == 0 and self.action == \"down\") or (self.y == self.MAX_VER_VAL and self.action == \"up\"):\n",
    "            return -2\n",
    "        elif (self.x, self.y) == (self.MAX_HOR_VAL-1, self.MAX_VER_VAL) and self.action == \"right\":\n",
    "            return 0\n",
    "        elif (self.x, self.y) == (self.MAX_HOR_VAL, self.MAX_VER_VAL-1) and self.action == \"up\":\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def take_action(self):\n",
    "#         # Check Errors\n",
    "#         if self.x < 0 or self.y < 0 or self.x > self.MAX_HOR_VAL or self.y > self.MAX_VER_VAL:\n",
    "#             print(\"Error in getting the state. Please execute the reset() function\")\n",
    "#             return (np.inf*-1)\n",
    "                \n",
    "        if self.x > -1 and self.x <= self.MAX_HOR_VAL:\n",
    "            if (self.action == \"left\" and self.x == 0) or (self.action == \"right\" and self.x == self.MAX_HOR_VAL):\n",
    "                self.x = self.x\n",
    "            elif(self.action == \"left\"):\n",
    "                self.x -= 1\n",
    "            elif(self.action == \"right\"):\n",
    "                self.x += 1\n",
    "            else:\n",
    "                self.x = self.x\n",
    "                \n",
    "        if self.y > -1 and self.y <= self.MAX_VER_VAL:\n",
    "            if (self.action == \"down\" and self.y == 0) or (self.action == \"up\" and self.y == self.MAX_HOR_VAL):\n",
    "                self.y = self.y\n",
    "            elif(self.action == \"down\"):\n",
    "                self.y -= 1\n",
    "            elif(self.action == \"up\"):\n",
    "                self.y += 1\n",
    "            else:\n",
    "                self.y = self.y\n",
    "                        \n",
    "        return [self.x, self.y]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def best_state_action_value(current_state):\n",
    "    max_val = np.inf*-1\n",
    "#     from IPython.core.debugger import Tracer; Tracer()() \n",
    "    for key in current_state.keys():\n",
    "        if current_state[key] > max_val:\n",
    "            max_val = current_state[key]\n",
    "            best_action = key\n",
    "    return best_action, max_val\n",
    "\n",
    "def current_state_to_string(state):\n",
    "    current_state = ''.join(str(int(e)) for e in state)\n",
    "    return current_state\n",
    "\n",
    "def get_all_states_as_strings():\n",
    "    states = []\n",
    "    for i in range(MAX_HOR_LENGTH):\n",
    "        for j in range(MAX_VER_LENGTH):\n",
    "            tmp = [i,j]\n",
    "            states.append(\"\".join(str(a) for a in tmp))\n",
    "    return states\n",
    "\n",
    "def initialize_Q():\n",
    "    Q = {}\n",
    "    states = get_all_states_as_strings()\n",
    "    for state in states:\n",
    "        Q[state] = {}\n",
    "        for i in range(4): #Number of actions = 4\n",
    "            Q[state][i] = np.random.uniform(-2,2,1)\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MAX_HOR_LENGTH = 4\n",
    "MAX_VER_LENGTH = 4\n",
    "MAX_STATES = MAX_HOR_LENGTH*MAX_HOR_LENGTH\n",
    "TOTAL_EPISODES = 1000\n",
    "SIM_RUN = 10\n",
    "SHOW_EVERY = 10\n",
    "OBSERVATION_SPACE = 2\n",
    "LEARNING_RATE = 0.05 # alpha in the literature\n",
    "DISCOUNT = 0.95 # gamma IN the literature\n",
    "EPSILON = 0.1\n",
    "START_EPSILON_DECAYING = 150\n",
    "END_EPSILON_DECAYING = 600\n",
    "epsilon_decay_value = EPSILON/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "\n",
    "Summed_reward = []\n",
    "\n",
    "for sim in range(SIM_RUN):\n",
    "    EPSILON = 0.1 \n",
    "    done = False\n",
    "    \n",
    "    agent = GridWorld_Env(MAX_HOR_LENGTH, MAX_VER_LENGTH)\n",
    "    Q_table = initialize_Q() \n",
    "\n",
    "    Total_running_reward = []\n",
    "    action_space = agent.action_space()\n",
    "    action_indexes = [i for i in range(len(action_space))]\n",
    "    cnt = 0\n",
    "    no_op = False\n",
    "    for episode in range(TOTAL_EPISODES):\n",
    "        done = False\n",
    "        current_state = agent.reset()\n",
    "        cnt += 1\n",
    "        total_episode_reward = 0\n",
    "        episode_length = 0\n",
    "\n",
    "        while not done:\n",
    "            current_state_str = current_state_to_string(current_state)\n",
    "            kind_of_selection_ = 'None'\n",
    "\n",
    "            if np.random.uniform() > EPSILON:\n",
    "                action, max_qt1 = best_state_action_value(Q_table[current_state_str])\n",
    "                kind_of_selection_ = 'Greedy'\n",
    "            else:\n",
    "                action = np.random.choice(action_indexes)\n",
    "                max_qt1 = Q_table[current_state_str][action]\n",
    "                kind_of_selection_ = 'Random'\n",
    "\n",
    "            next_state, reward, done, no_op, episode_length = agent.step(action_space[action])\n",
    "            total_episode_reward += reward\n",
    "            Q_table[current_state_str][action] += LEARNING_RATE*(reward + DISCOUNT*max_qt1 - Q_table[current_state_str][action])\n",
    "#             print(f'current state : {current_state}. Action : {action_space[action]}. Next state: {next_state}. Kind of Sel: {kind_of_selection_}')\n",
    "            current_state = next_state;\n",
    "            cnt+=1\n",
    "\n",
    "        Total_running_reward.append(total_episode_reward)\n",
    "        \n",
    "        if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "            EPSILON -= epsilon_decay_value\n",
    "    \n",
    "    if sim == 0:\n",
    "        Summed_reward = Total_running_reward\n",
    "    else: \n",
    "        Summed_reward = np.vstack((Summed_reward,Total_running_reward))\n",
    "        \n",
    "    if sim % SHOW_EVERY == 0:\n",
    "        print(sim)\n",
    "\n",
    "# Displaying average reward\n",
    "\n",
    "df = pd.DataFrame(Summed_reward)\n",
    "Mean_total_reward = df.mean()\n",
    "Mean_total_reward\n",
    "# print('--------------------------')\n",
    "# print(f'Numer of steps per episode : {episode_length}. Reward : {Total_running_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def best_state_action_value_(current_state):\n",
    "    max_val = np.inf*-1\n",
    "#     from IPython.core.debugger import Tracer; Tracer()() \n",
    "    for key in current_state.keys():\n",
    "        if current_state[key] > max_val:\n",
    "            max_val = current_state[key]\n",
    "            best_action = key\n",
    "    return best_action, max_val\n",
    "\n",
    "Total_running_reward = []\n",
    "action_space = agent.action_space()\n",
    "action_indexes = [i for i in range(len(action_space))]\n",
    "cnt = 0\n",
    "no_op = False\n",
    "\n",
    "done = False\n",
    "current_state = agent.reset()\n",
    "total_episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "    current_state_str = current_state_to_string(current_state)\n",
    "    action, max_qt1 = best_state_action_value_(Q_table[current_state_str])\n",
    "\n",
    "    next_state, reward, done, no_op, episode_length = agent.step(action_space[action])\n",
    "    total_episode_reward += reward\n",
    "    Q_table[current_state_str][action] += LEARNING_RATE*(reward + DISCOUNT*max_qt1 - Q_table[current_state_str][action])\n",
    "    print(f'current state : {current_state}. Action : {action_space[action]}. Next state: {next_state}. Kind of Sel: {kind_of_selection_}')\n",
    "    current_state = next_state;\n",
    "    cnt+=1\n",
    "\n",
    "Total_running_reward.append(total_episode_reward)\n",
    "\n",
    "print(f'Numer of steps per episode : {episode_length}. Reward : {Total_running_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(Mean_total_reward)\n",
    "plt.grid()\n",
    "plt.title('Mean reward after 50 simulation of 1000 Episode each')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards / Costs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average\n",
    "N = 100\n",
    "cumsum, moving_aves = [0], []\n",
    "\n",
    "for i, x in enumerate(Total_running_reward, 1):\n",
    "    cumsum.append(cumsum[i-1] + x)\n",
    "    if i>=N:\n",
    "        moving_ave = (cumsum[i] - cumsum[i-N])/N\n",
    "        #can do stuff with moving_ave here\n",
    "        moving_aves.append(moving_ave)\n",
    "\n",
    "plt.plot(moving_aves)\n",
    "plt.title('Moving Average')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_reward = 0\n",
    "# current_state = agent.reset()\n",
    "\n",
    "Q_ = Q_table\n",
    "Test_reward_mat = []\n",
    "for ep in range(1):\n",
    "    done = False\n",
    "    current_state = agent.reset()\n",
    "    Test_reward = 0\n",
    "    print('-------------------\\n')\n",
    "    while not done:\n",
    "        current_state_str = current_state_to_string(current_state)\n",
    "        action, max_qt1 = best_state_action_value(Q_[current_state_str])\n",
    "        next_state, reward, done, no_op, episode_length = agent.step(action_space[action])\n",
    "        Test_reward+=reward\n",
    "        print(f'Current State : {current_state}, Action: {action_space[action]}, Next_State : {next_state}')\n",
    "        current_state = next_state\n",
    "        \n",
    "    Test_reward_mat.append(Test_reward)\n",
    "\n",
    "for i in range(len(Test_reward_mat)):\n",
    "    print(f'Test Reward for sim {i} is : {Test_reward_mat[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
